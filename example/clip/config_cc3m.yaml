model:
    type: clip_vitb32
    kwargs:
        image_encode:
            embed_dim: 512
        text_encode:
            bpe_path: 'text_info/bpe_simple_vocab_16e6.txt.gz'
            text_encode_type: Transformer #Transformer,Bert,GPT2
            text_model_utils:
                random: False
                freeze: False
            embed_dim: 512
        clip:
            use_allgather: True

grad_clip:
    type: logit_scale_param_value
    value: 3
    max_value: 6


optimizer:
    type: AdamW
    kwargs:
        lr: 0.0001  # 5e-4
        weight_decay: 0.1
        betas: [0.9, 0.98]
        amsgrad: False
        eps: 0.00000001

    pconfig:
        bn_w:
            weight_decay: 0
        bn_b:
            weight_decay: 0
        ln_w:
            weight_decay: 0
        ln_b:
            weight_decay: 0
        bias:
            weight_decay: 0
        logit_scale:
            # lr: 0.0001  # not useful
            weight_decay: 0


lr_scheduler:
    type: Cosine
    kwargs:
        base_lr: 0.0001
        warmup_lr: 0.001
        min_lr: 0.0
        warmup_steps: 1000
        max_iter: 72000


data:
    train:
        epoch: 30
        data_path: data/cc3m/{00000..00331}.tar
        transforms: MOCOV2_single
        num_samples: 2769025
        num_shards: 331
        workers: 5
        batch_size: 256

    test:
        data_fold: /research/img_dataset
        sc_image_root: /research/sc_image_root
        sc_data_root: /research/sc_data_root


saver:
    print_freq: 100
    val_freq: 4000
    save_freq: 12000
    save_many: True